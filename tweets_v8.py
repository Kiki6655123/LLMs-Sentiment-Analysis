import time
import json
import pandas as pd
from datetime import datetime
import random
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from langdetect import detect
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import emoji
import re
import uuid
from snownlp import SnowNLP
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# è®¾ç½®Chromeé©±åŠ¨è·¯å¾„
chrome_driver_path = 'D:/Program Files/Android/chromedriver-win64/chromedriver.exe'
chrome_options = Options()
user_data_dir = r'C:\Users\Lenovo\AppData\Local\Google\Chrome\User Data'
chrome_options.add_argument(f'--user-data-dir={user_data_dir}')
chrome_options.add_argument('--profile-directory=Default')

# åˆå§‹åŒ–WebDriver
service = Service(chrome_driver_path)
driver = webdriver.Chrome(service=service, options=chrome_options)
wait = WebDriverWait(driver, 20)

def load_cookies():
    """åŠ è½½Twitter/X cookies"""
    driver.get("https://x.com/")
    time.sleep(random.uniform(5, 15))
    cookies = [
        {"name": "auth_token", "value": "1196bcad107d1f758c9de5156a8325060750ea4f", "domain": ".x.com", "path": "/"},
        {"name": "gt", "value": "1898999816707031440", "domain": ".x.com", "path": "/"},
        {"name": "guest_id", "value": "v1%3A174159317962469607", "domain": ".x.com", "path": "/"},
        {"name": "twid", "value": "u%3D1871808817249624065", "domain": ".x.com", "path": "/"},
        {"name": "lang", "value": "en", "domain": ".x.com", "path": "/"},
        {"name": "night_mode", "value": "2", "domain": ".x.com", "path": "/"},
        {"name": "att", "value": "1-K93zYDZuMb1vYSIH1JjJGPY1GloCHAUBgjJYmG5e", "domain": ".x.com", "path": "/"},
        {"name": "ct0", "value": "3f49edfd1430849b6a92d2f73e826b445aa54c4c6f589e5732cbcd44f1a47e57dbadfcc7bb0d1a25f3cf49e587be205b60c390e8b019b0e48b0b73c314f1d099d1e13d7a56add0d6418ac74e88ef527f", "domain": ".x.com", "path": "/"},
        {"name": "guest_id_ads", "value": "v1%3A174159317962469607", "domain": ".x.com", "path": "/"},
        {"name": "guest_id_marketing", "value": "v1%3A174159317962469607", "domain": ".x.com", "path": "/"},
        {"name": "kdt", "value": "QAKeSMHW4NGqipuMqhxwRgjLuOM3cMUhAYUa1LRR", "domain": ".x.com", "path": "/"},
        {"name": "personalization_id", "value": "v1_RZQCMgmB/TXywdb0caTBAg==", "domain": ".x.com", "path": "/"}
    ]
    for cookie in cookies:
        try:
            driver.delete_cookie(cookie['name'])
            driver.add_cookie(cookie)
            logging.info(f"æˆåŠŸæ·»åŠ cookie: {cookie['name']}")
        except Exception as e:
            logging.warning(f"æ·»åŠ cookie {cookie['name']} å¤±è´¥: {str(e)}")
    driver.refresh()
    time.sleep(random.uniform(5, 15))
    # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
    current_url = driver.current_url
    logging.debug(f"å½“å‰URL: {current_url}")
    if "login" in current_url or "x.com" not in current_url:
        logging.error("Cookies æ— æ•ˆï¼Œæœªèƒ½ç™»å½• X.com")
        return False
    else:
        logging.info("Cookies æœ‰æ•ˆï¼Œå·²æˆåŠŸç™»å½• X.com")
        return True

# æ–‡åŒ–ç‰¹å¾æå–ï¼ˆä¿šè¯­è¯å…¸ï¼‰
slang_dict = {
    "lit": "å…´å¥‹", "savage": "é‡è›®", "fam": "å®¶äºº", "slay": "å¹²å¾—æ¼‚äº®", "yolo": "ä½ åªæ´»ä¸€æ¬¡",
    "bruh": "å…„å¼Ÿ", "lol": "ç¬‘", "omg": "å¤©å“ª", "cool": "é…·", "trash": "åƒåœ¾",
    "666": "å‰å®³", "å“ˆå“ˆ": "ç¬‘", "å‘µå‘µ": "å˜²ç¬‘", "ç‰›é€¼": "å‰å®³", "è¾£é¸¡": "åƒåœ¾",
    "æ— è¯­": "æ— è¯å¯è¯´", "æ„ŸåŠ¨": "æ„ŸåŠ¨", "å‘çˆ¹": "å‘äºº", "ç¬‘æ­»": "ç¬‘æ­»æˆ‘äº†",
    "ä¹±æ": "èƒ¡ä¹±æ", "å‡ æŠŠç”¨": "æ²¡ç”¨", "åœŸç‹—": "è´¬ä¹‰ç§°å‘¼", "éš¾å—": "ä¸èˆ’æœ"
}

# è‡ªå®šä¹‰æƒ…æ„Ÿå…³é”®è¯
positive_keywords = ["å‰å®³", "å¥½", "æ²¡é”™", "èµšé’±", "æ–¹ä¾¿", "èµ‹èƒ½", "ç»æµå‘å±•", "æ”¯æŒ", "cool", "great", "awesome"]
negative_keywords = ["ä¹±æ", "éš¾å—", "åƒåœ¾", "å‘çˆ¹", "ä¸çŸ¥", "ç–¾è‹¦", "åå¯¹", "æ²¡ç”¨", "é«˜æ½®", "å‡ æŠŠç”¨", "å’‹åœ°", "ä¸çˆ±å›½", "æ— å›½ç•Œ", "trash", "bad"]

def extract_abbreviations(text):
    """æå–ç¼©å†™"""
    return re.findall(r'\b[A-Za-z]{2,5}\b', text)

def extract_slangs(text):
    """æå–ä¿šè¯­"""
    words = re.split(r'\s+', text.lower())
    return [word for word in words if word in slang_dict]

def extract_features(text):
    """æå–æ–‡åŒ–ç‰¹å¾ï¼ˆè¡¨æƒ…ã€ç¼©å†™ã€ä¿šè¯­ï¼‰"""
    emojis_list = [e['emoji'] for e in emoji.emoji_list(text)]
    abbreviations = extract_abbreviations(text)
    slangs = extract_slangs(text)
    return {
        'emojis': emojis_list,
        'abbreviations': abbreviations,
        'slangs': slangs
    }

def get_tweet_data(tweet_element):
    try:
        text_element = tweet_element.find_element(By.XPATH, './/div[@data-testid="tweetText"]')
        text = ""
        for child in text_element.find_elements(By.XPATH, './span | ./img'):
            if child.tag_name == 'img':
                emoji_char = child.get_attribute('alt') or ''
                text += emoji_char
            else:
                text += child.text
        text = text.strip()

        time_element = tweet_element.find_element(By.XPATH, './/time')
        timestamp = time_element.get_attribute('datetime')
        author_element = tweet_element.find_element(By.XPATH, './/div[@data-testid="User-Name"]')
        author = author_element.text.split('\n')[0]
        replies = retweets = likes = "0"
        try:
            replies = tweet_element.find_element(By.XPATH, './/div[@data-testid="reply"]').text or "0"
            retweets = tweet_element.find_element(By.XPATH, './/div[@data-testid="retweet"]').text or "0"
            likes = tweet_element.find_element(By.XPATH, './/div[@data-testid="like"]').text or "0"
        except:
            pass
        cultural_features = extract_features(text)
        logging.debug(f"æå–æ¨æ–‡: {text[:20]}... Emoji: {cultural_features['emojis']}")
        return {
            "text": text,
            "timestamp": timestamp,
            "author": author,
            "engagement": {"replies": replies, "retweets": retweets, "likes": likes},
            "cultural_features": cultural_features
        }
    except Exception as e:
        logging.error(f"æå–æ¨æ–‡æ•°æ®æ—¶å‡ºé”™: {e}")
        return None


def analyze_sentiment(text):
    try:
        detected_lang = detect(text)
        logging.debug(f"æ£€æµ‹è¯­è¨€: {detected_lang}")
        if detected_lang not in ['en', 'zh']:
            detected_lang = 'en'

        sentiment_label = "neutral"
        confidence = 0.0

        if detected_lang == 'en':
            analyzer = SentimentIntensityAnalyzer()
            scores = analyzer.polarity_scores(text)
            logging.debug(f"è‹±æ–‡æƒ…æ„Ÿåˆ†æ•°: {scores}")
            compound = scores['compound']
            if compound >= 0.1:
                sentiment_label = "positive"
                confidence = compound
            elif compound <= -0.1:
                sentiment_label = "negative"
                confidence = -compound
            else:
                sentiment_label = "neutral"
                confidence = abs(compound)
        elif detected_lang == 'zh':
            text_cleaned = re.sub(r'[^\w\s]', '', text)
            logging.debug(f"æ¸…æ´—åæ–‡æœ¬: {text_cleaned}")
            snownlp_obj = SnowNLP(text_cleaned)
            sentiment_score = snownlp_obj.sentiments
            logging.debug(f"åŸå§‹æƒ…æ„Ÿåˆ†æ•°: {sentiment_score}")
            positive_count = sum(1 for word in positive_keywords if word in text)
            negative_count = sum(1 for word in negative_keywords if word in text)
            sentiment_boost = (positive_count - negative_count) * 0.3
            logging.debug(f"æ­£å‘è¯æ•°: {positive_count}, è´Ÿå‘è¯æ•°: {negative_count}, è°ƒæ•´å€¼: {sentiment_boost}")
            adjusted_score = min(max(sentiment_score + sentiment_boost, 0.0), 1.0)
            logging.debug(f"è°ƒæ•´åæƒ…æ„Ÿåˆ†æ•°: {adjusted_score}")
            if adjusted_score > 0.8:
                sentiment_label = "positive"
                confidence = adjusted_score
            elif adjusted_score < 0.2:
                sentiment_label = "negative"
                confidence = 1 - adjusted_score
            else:
                sentiment_label = "neutral"
                confidence = min(abs(adjusted_score - 0.5) * 4, 1.0)

        return detected_lang, sentiment_label, confidence
    except Exception as e:
        logging.error(f"æƒ…æ„Ÿåˆ†æå‡ºé”™: {e}")
        return "unknown", "neutral", 0.0

def scroll_and_collect_tweets(query, max_scrolls=50):
    try:
        driver.get(query)
        time.sleep(random.uniform(5, 15))  # åœé¡¿æ—¶é—´æ”¹ä¸º 5-15 ç§’
        driver.execute_script("return document.readyState === 'complete';")
        logging.debug(f"è®¿é—®æŸ¥è¯¢: {query}")
        tweets_collected = set()
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0

        while scroll_count < max_scrolls:
            try:
                wait.until(EC.presence_of_all_elements_located((By.XPATH, '//article[@data-testid="tweet"]')))
                tweets = driver.find_elements(By.XPATH, '//article[@data-testid="tweet"]')
                logging.debug(f"æ‰¾åˆ° {len(tweets)} æ¡æ¨æ–‡")
                for tweet in tweets:
                    tweet_data = get_tweet_data(tweet)
                    if tweet_data and tweet_data['text'] not in tweets_collected:
                        tweets_collected.add(tweet_data['text'])
                        yield tweet_data
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(5, 15))
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    logging.info("é¡µé¢é«˜åº¦æœªå˜åŒ–ï¼Œåœæ­¢æ»šåŠ¨")
                    break
                last_height = new_height
                scroll_count += 1
            except Exception as e:
                logging.error(f"æ»‘åŠ¨æ”¶é›†æ—¶å‡ºé”™: {e}")
                break
    except Exception as e:
        logging.error(f"è®¿é—®æŸ¥è¯¢ {query} æ—¶å‡ºé”™: {e}")
        return

# ä¸»é¢˜å…³é”®è¯åˆ—è¡¨
topics_keywords = {
    "Relationships": {
        "en": ["love", "heartbreak", "dating", "marriage", "friendship", "divorce", "cheating", "trust issues"],
        "emojis": ["â¤ï¸", "ğŸ’”", "ğŸ’‘", "ğŸ’", "ğŸ‘«", "ğŸ’", "ğŸ˜¢", "ğŸ’"]
    },
    "Mental Health": {
        "en": ["depression", "anxiety", "self-care", "therapy", "loneliness", "suicide prevention", "PTSD", "stress"],
        "emojis": ["ğŸ˜”", "ğŸ’™", "ğŸ§ ", "ğŸ’†â€â™‚ï¸", "ğŸŒ¿", "ğŸ˜¢", "ğŸ˜", "ğŸ˜­"]
    },
    "Social Issues": {
        "en": ["racism", "gender equality", "feminism", "LGBTQ rights", "bullying", "human rights", "poverty", "homelessness"],
        "emojis": ["âœŠ", "â™€ï¸", "ğŸ³ï¸â€ğŸŒˆ", "ğŸ’”", "âš–ï¸", "ğŸ•Šï¸", "ğŸ¤", "ğŸ˜¡"]
    },
    "Politics & Controversy": {
        "en": ["elections", "war", "violence", "protests", "discrimination", "justice", "corruption", "government"],
        "emojis": ["âš–ï¸", "ğŸ—³ï¸", "ğŸ”¥", "ğŸš”", "ğŸ’¬", "ğŸ¤¬", "ğŸ˜¡", "ğŸ›ï¸"]
    },
    "Work & Stress": {
        "en": ["burnout", "job loss", "career growth", "work-life balance", "unemployment", "toxic workplace", "overwork"],
        "emojis": ["ğŸ’¼", "ğŸ˜“", "ğŸ“‰", "ğŸ”¥", "ğŸ’ª", "ğŸ§˜", "ğŸ˜¤", "ğŸ˜­"]
    },
    "Entertainment & Pop Culture": {
        "en": ["celebrity gossip", "scandal", "music", "movies", "fan wars", "drama", "cancel culture"],
        "emojis": ["ğŸ¬", "ğŸ¤", "ğŸŒŸ", "ğŸ”¥", "ğŸ˜±", "ğŸ’£", "ğŸ­", "ğŸ˜¡"]
    },
    "Online & Social Media": {
        "en": ["cancel culture", "hate speech", "trolling", "viral trends", "influencers", "cyberbullying", "online harassment"],
        "emojis": ["ğŸ’»", "ğŸ“±", "ğŸ”¥", "ğŸ’¬", "ğŸ™„", "ğŸ˜¡", "ğŸ˜ ", "ğŸ›‘"]
    },
    "Family & Parenting": {
        "en": ["parenting", "childhood trauma", "family conflict", "motherhood", "fatherhood", "abuse", "child neglect"],
        "emojis": ["ğŸ‘¶", "ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦", "ğŸ’", "ğŸ˜­", "ğŸ˜¡", "ğŸ¼", "ğŸ’”", "ğŸ˜¢"]
    },
    "Health & Well-being": {
        "en": ["COVID-19", "vaccine", "chronic illness", "weight loss", "body positivity", "eating disorders", "disability"],
        "emojis": ["ğŸ’‰", "ğŸ˜·", "ğŸ©º", "ğŸ‹ï¸", "ğŸ¥—", "ğŸ’ª", "ğŸ¤•", "ğŸ¤’"]
    },
    "Violence & Crime": {
        "en": ["gun violence", "domestic abuse", "sexual harassment", "kidnapping", "assault", "murder", "human trafficking"],
        "emojis": ["ğŸ”«", "ğŸ†˜", "ğŸš”", "ğŸ’€", "âš–ï¸", "ğŸ˜¡", "ğŸ˜­", "ğŸ‘®"]
    },
    "Financial Struggles": {
        "en": ["debt", "bankruptcy", "poverty", "inflation", "economic crisis", "job insecurity", "high cost of living"],
        "emojis": ["ğŸ’°", "ğŸ“‰", "ğŸ’¸", "ğŸ¦", "ğŸ˜", "ğŸ˜­", "ğŸ“‰", "ğŸ˜ "]
    },
    "Education & Student Life": {
        "en": ["exam stress", "student debt", "bullying", "peer pressure", "dropout", "mental pressure", "academic failure"],
        "emojis": ["ğŸ“š", "âœï¸", "ğŸ’­", "ğŸ˜­", "ğŸ˜“", "ğŸ˜", "ğŸ“–", "ğŸ’¡"]
    },
    "Technology & AI": {
        "en": ["AI ethics", "privacy concerns", "data security", "automation", "job displacement", "social media addiction"],
        "emojis": ["ğŸ¤–", "ğŸ”’", "ğŸ’¾", "ğŸ“¡", "ğŸ“±", "ğŸ§ ", "ğŸ˜µ", "ğŸ”"]
    },
    "Addiction & Recovery": {
        "en": ["drug addiction", "alcoholism", "rehabilitation", "smoking", "gambling", "internet addiction", "relapse"],
        "emojis": ["ğŸ·", "ğŸš¬", "ğŸ’Š", "ğŸ’”", "ğŸ˜", "ğŸ˜­", "ğŸš‘", "ğŸ”„"]
    },
    "Discrimination & Prejudice": {
        "en": ["ageism", "sexism", "ableism", "xenophobia", "homophobia", "transphobia", "religious discrimination"],
        "emojis": ["âš–ï¸", "â™€ï¸", "ğŸ³ï¸â€ğŸŒˆ", "ğŸš«", "ğŸ›‘", "ğŸ˜¡", "ğŸ¤¬", "ğŸ˜¢"]
    },
    "Personal Growth & Self-Improvement": {
        "en": ["motivation", "failure", "resilience", "self-esteem", "goal setting", "mental discipline"],
        "emojis": ["ğŸŒ±", "ğŸ’ª", "ğŸ“ˆ", "ğŸ§˜", "ğŸ¯", "ğŸ”„", "ğŸ†", "âœ¨"]
    }
}
languages = ["en"]

def get_tweets_by_topic_and_language(topics_keywords, languages):
    all_tweets = []
    if not load_cookies():
        logging.error("ç™»å½•å¤±è´¥ï¼Œåœæ­¢é‡‡é›†")
        return all_tweets
    for topic, keywords in topics_keywords.items():
        for lang in languages:
            query_keywords = keywords[lang] + keywords["emojis"]
            for keyword in query_keywords:
                query = f"https://x.com/search?q={keyword}%20lang%3A{lang}%20-is%3Aretweet%20-is%3Areply&src=typed_query&f=live"
                logging.info(f"æ­£åœ¨é‡‡é›†: ä¸»é¢˜={topic}, è¯­è¨€={lang}, å…³é”®è¯={keyword}")
                tweet_count = 0
                try:
                    for tweet_data in scroll_and_collect_tweets(query, max_scrolls=5):
                        if tweet_data:
                            detected_lang, sentiment_label, confidence = analyze_sentiment(tweet_data['text'])
                            tweet_data.update({
                                "id": f"tweet_{uuid.uuid4().hex}",
                                "topic": topic,
                                "query_language": lang,
                                "language": {"original": lang, "detected": detected_lang, "contains_mixed": detected_lang != lang},
                                "sentiment": {"overall": sentiment_label, "confidence": confidence},
                                "metadata": {
                                    "annotator": "auto",
                                    "annotation_time": datetime.now().strftime('%Y-%m-%d'),
                                    "review_status": "pending"
                                }
                            })
                            total_engagement = sum(
                                int(tweet_data['engagement'][k]) for k in ['replies', 'retweets', 'likes'] if tweet_data['engagement'][k].isdigit())
                            tweet_data['engagement']['total_engagement'] = total_engagement
                            all_tweets.append(tweet_data)
                            tweet_count += 1
                            if tweet_count >= 50:
                                break
                except Exception as e:
                    logging.error(f"é‡‡é›†æ¨æ–‡æ—¶å‡ºé”™: ä¸»é¢˜={topic}, è¯­è¨€={lang}, å…³é”®è¯={keyword}, é”™è¯¯={e}")
                logging.info(f"å®Œæˆé‡‡é›†: ä¸»é¢˜={topic}, è¯­è¨€={lang}, å…³é”®è¯={keyword}, å…±{tweet_count}æ¡")
                time.sleep(random.uniform(2, 5))
    return all_tweets

def save_data(tweets_data, base_filename='twitter_data'):
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_data = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'total_tweets': len(tweets_data),
            'language_distribution': {},
            'topic_distribution': {}
        },
        'tweets': tweets_data
    }
    for tweet in tweets_data:
        lang = tweet['language']['detected']
        topic = tweet['topic']
        output_data['metadata']['language_distribution'][lang] = output_data['metadata']['language_distribution'].get(lang, 0) + 1
        output_data['metadata']['topic_distribution'][topic] = output_data['metadata']['topic_distribution'].get(topic, 0) + 1

    json_filename = f'{base_filename}_{timestamp}.json'
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    simplified_data = [
        {
            "label": {"positive": 1, "negative": 0, "neutral": 2}[tweet['sentiment']['overall']],
            "cleaned_review": re.sub(r'[\n\r]+', ' ', tweet['text']).strip()
        } for tweet in tweets_data
    ]
    df = pd.DataFrame(simplified_data)
    csv_filename = f'{base_filename}_{timestamp}_simplified.csv'
    df.to_csv(csv_filename, index=False, encoding='utf-8-sig')

    return json_filename, csv_filename

# æ‰§è¡Œé‡‡é›†
try:
    tweets = get_tweets_by_topic_and_language(topics_keywords, languages)
    json_filename, csv_filename = save_data(tweets)
    logging.info(f"æ•°æ®å·²ä¿å­˜ä¸º {json_filename} å’Œ {csv_filename}")
except Exception as e:
    logging.error(f"æ‰§è¡Œé‡‡é›†æ—¶å‡ºé”™: {e}")
finally:
    driver.quit()