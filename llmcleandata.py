import os
import json
import csv
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import gc
import logging

# é…ç½®æ—¥å¿—
LOG_FILE = "/gemini/code/script.log"
os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# é…ç½®
MODEL_PATH = "/gemini/code/DeepSeek-R1-Distill-Qwen-7B"
OUTPUT_DIR = "/gemini/code/output"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 16

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        low_cpu_mem_usage=True,
        torch_dtype=torch.float16
    )
    model.to(DEVICE)
    logging.info(f"Model loaded on device: {DEVICE}")
except Exception as e:
    logging.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit(1)

# åˆ†å‰²é•¿æ–‡æœ¬ä¸ºå¥å­
def split_long_text(text, min_length=10):
    sentences = re.split(r'[.!?]\s+|\n+', text)
    split_texts = [sent.strip() + '.' for sent in sentences if len(sent.strip()) >= min_length]
    return split_texts if split_texts else [text]

# ç­›é€‰æ–‡æœ¬æ‰¹æ¬¡
def screen_text_batch(texts):
    if not texts:
        logging.warning("æ‰¹æ¬¡æ–‡æœ¬ä¸ºç©º")
        return []
    
    prompt = """
ä»¥ä¸‹æ˜¯éœ€è¦åˆ†ç±»çš„æ–‡æœ¬ã€‚ä»»åŠ¡æ˜¯å°†æ¯ä¸ªæ–‡æœ¬åˆ†ç±»ä¸º 'positive'ã€'negative' æˆ– 'neutral/insignificant'ã€‚
- ä¸¥æ ¼è¿”å›æ ¼å¼ï¼š["class1", "class2", ...]
- ç¦æ­¢æ·»åŠ ä»»ä½•æ¨ç†ã€è¯´æ˜æˆ–å…¶ä»–å¤šä½™æ–‡æœ¬ã€‚
- ç›´æ¥åŸºäºä»¥ä¸‹è§„åˆ™åˆ†ç±»ï¼š
  - 'neutral/insignificant'ï¼šæ— æ„ä¹‰çš„åƒåœ¾æ–‡æœ¬ï¼ˆå¦‚ '#@$%'ã€'ok'ï¼‰æˆ–æ— æƒ…æ„Ÿã€æ— æ„è§çš„äº‹å®ï¼ˆå¦‚ 'ç°åœ¨æ˜¯ä¸‹åˆ3ç‚¹'ï¼‰ã€‚
  - 'positive'ï¼šåŒ…å«ç§¯ææƒ…æ„Ÿã€èµç¾ã€ä¿ƒé”€è¯­æ°”æˆ–è¡¨æƒ…ç¬¦å·ï¼ˆå¦‚ 'å¥½'ã€'æ£’'ã€'æœ€ä½³'ã€'ğŸ˜Š'ã€'ğŸš€'ï¼‰ã€‚
  - 'negative'ï¼šåŒ…å«æ¶ˆææƒ…æ„Ÿã€æ‰¹è¯„æˆ–è´Ÿé¢è¯­æ°”ï¼ˆå¦‚ 'å'ã€'ç³Ÿç³•'ã€'æ‰¹è¯„'ã€'ğŸ˜¢'ï¼‰ã€‚
  - å¦‚æœ‰ç–‘é—®ï¼Œä¼˜å…ˆé€‰æ‹© 'positive' æˆ– 'negative'ã€‚
  - æ³¨æ„ï¼šä»»ä½•åŒ…å«æƒ…æ„Ÿè¯ã€è¡¨æƒ…ç¬¦å·æˆ–è¯­æ°”çš„æ–‡æœ¬ä¸å¾—åˆ†ç±»ä¸º 'neutral/insignificant'ã€‚

è¾“å…¥æ–‡æœ¬ï¼š
"""
    for i, text in enumerate(texts):
        prompt += f"Text {i+1}: \"{text}\"\n"
    prompt += "\nè¾“å‡ºï¼š"

    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.001,
            top_p=0.9,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()
    logging.info(f"ç­›é€‰è¾“å…¥æ‰¹æ¬¡: {texts}")
    logging.info(f"åŸå§‹ç­›é€‰è¾“å‡º: {response}")
    
    match = re.search(r'\[(.*?)\]', response, re.DOTALL)
    cleaned_response = match.group(0) if match else response
    logging.info(f"æ¸…ç†åçš„ç­›é€‰è¾“å‡º: {cleaned_response}")
    
    try:
        if match:
            classifications = [cls.strip(' "\',').replace('"', '') for cls in match.group(1).split(',')]
        else:
            if cleaned_response.startswith('[') and cleaned_response.endswith(']'):
                classifications = json.loads(cleaned_response)
            else:
                raise ValueError("è¾“å‡ºæ ¼å¼æ— æ•ˆ")
        
        classifications = classifications[:len(texts)]
        if len(classifications) < len(texts):
            logging.warning(f"åˆ†ç±»æ•°é‡ä¸è¶³ ({len(classifications)}/{len(texts)})ï¼Œç”¨ 'neutral/insignificant' å¡«å……")
            classifications += ["neutral/insignificant"] * (len(texts) - len(classifications))
        
        for i, (text, cls) in enumerate(zip(texts, classifications)):
            if cls == "neutral/insignificant":
                if any(kw in text.lower() for kw in ['å¥½', 'æ£’', 'æœ€ä½³', 'exciting', 'amazing', 'ğŸ˜Š', 'ğŸš€']) or '!' in text:
                    classifications[i] = "positive"
                elif any(kw in text.lower() for kw in ['å', 'ç³Ÿç³•', 'risk', 'criticism', 'ğŸ˜¢']):
                    classifications[i] = "negative"
        
        logging.info(f"è§£æåçš„åˆ†ç±»: {classifications}")
        return classifications
    except Exception as e:
        logging.error(f"è§£æç­›é€‰è¾“å‡ºé”™è¯¯: {response}. é”™è¯¯: {e}")
        return ["neutral/insignificant"] * len(texts)

# æƒ…æ„Ÿåˆ†ææ‰¹æ¬¡
def analyze_sentiment_batch(texts, classifications):
    filtered_texts = [(text, cls) for text, cls in zip(texts, classifications) if cls in ["positive", "negative"]]
    if not filtered_texts:
        logging.warning("æ— éœ€æƒ…æ„Ÿåˆ†æçš„æ–‡æœ¬")
        return []

    prompt = """
ä»¥ä¸‹æ˜¯éœ€è¦æƒ…æ„Ÿåˆ†æçš„æ–‡æœ¬ã€‚ä»»åŠ¡æ˜¯å¯¹æ¯ä¸ªæ ‡è®°ä¸º 'positive' æˆ– 'negative' çš„æ–‡æœ¬ç”Ÿæˆæƒ…æ„Ÿåˆ†æã€‚
- ä¸¥æ ¼è¿”å›æ ¼å¼ï¼š[{"cleaned_text": "text", "sentiment": "positive/negative", "reasoning": "..."}, ...]
- ç¦æ­¢æ·»åŠ ä»»ä½•å¤šä½™æ–‡æœ¬ï¼Œä»…åŸºäºè¾“å…¥æ–‡æœ¬å’Œåˆ†ç±»ç”Ÿæˆç»“æœã€‚
- reasoning éœ€ç®€æ´ï¼Œè¯´æ˜æƒ…æ„Ÿæ¥æºï¼ˆå¦‚æƒ…æ„Ÿè¯ã€è¡¨æƒ…ç¬¦å·ã€è¯­æ°”ï¼‰ã€‚

è¾“å…¥æ–‡æœ¬ï¼š
"""
    for i, (text, cls) in enumerate(filtered_texts):
        prompt += f"Text {i+1}: \"{text}\" Classification: \"{cls}\"\n"
    prompt += "\nè¾“å‡ºï¼š"

    logging.info(f"æƒ…æ„Ÿåˆ†æè¾“å…¥: {filtered_texts}")
    
    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=400,
            temperature=0.001,
            top_p=0.9,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()
    logging.info(f"æƒ…æ„Ÿåˆ†æè¾“å‡ºæ‰¹æ¬¡: {response}")
    
    match = re.search(r'\[.*\]', response, re.DOTALL)
    cleaned_response = match.group(0) if match else response
    logging.info(f"æ¸…ç†åçš„æƒ…æ„Ÿåˆ†æè¾“å‡º: {cleaned_response}")
    
    try:
        json_data = json.loads(cleaned_response)
        if len(json_data) != len(filtered_texts):
            logging.warning(f"æƒ…æ„Ÿåˆ†æè¾“å‡ºæ•°é‡ ({len(json_data)}) ä¸è¾“å…¥æ•°é‡ ({len(filtered_texts)}) ä¸åŒ¹é…")
        return json_data
    except Exception as e:
        logging.error(f"è§£ææƒ…æ„Ÿåˆ†æè¾“å‡ºé”™è¯¯: {response}. é”™è¯¯: {e}")
        return []

# ä¸»å¤„ç†å‡½æ•°
def process_file(file_path, batch_size=BATCH_SIZE):
    filename = os.path.basename(file_path).replace('.json', '')
    logging.info(f"å¤„ç†æ–‡ä»¶: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if 'tweets' not in data:
        logging.error(f"{filename} ä¸­æ²¡æœ‰ 'tweets' å­—æ®µ")
        return
    raw_tweets = [tweet.get('text', '') for tweet in data['tweets'] if 'text' in tweet]
    if not raw_tweets:
        logging.error(f"{filename} ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ–‡æœ¬æ•°æ®")
        return
    
    logging.info(f"åŸå§‹æ¨æ–‡æ•°é‡: {len(raw_tweets)}")
    all_texts = []
    text_mapping = {}
    for i, tweet in enumerate(raw_tweets):
        split_texts = split_long_text(tweet)
        all_texts.extend(split_texts)
        text_mapping[i] = split_texts
    
    logging.info(f"åˆ†å‰²åæ–‡æœ¬æ•°é‡: {len(all_texts)}")
    total_batches = (len(all_texts) + batch_size - 1) // batch_size
    logging.info(f"æ€»æ‰¹æ¬¡æ•°: {total_batches}")
    
    all_sentiment_results = []
    all_csv_rows = []
    
    for batch_num in tqdm(range(total_batches), desc=f"å¤„ç† {filename}"):
        start_idx = batch_num * batch_size
        end_idx = min(start_idx + batch_size, len(all_texts))
        batch_texts = all_texts[start_idx:end_idx]
        
        classifications = screen_text_batch(batch_texts)
        sentiment_results = analyze_sentiment_batch(batch_texts, classifications)
        
        logging.info(f"æ‰¹æ¬¡ {batch_num} åˆ†ç±»ç»“æœ: {classifications}")
        logging.info(f"æ‰¹æ¬¡ {batch_num} æƒ…æ„Ÿåˆ†æç»“æœ: {sentiment_results}")
        
        all_sentiment_results.extend(sentiment_results)
        
        for text, cls in zip(batch_texts, classifications):
            if cls in ["positive", "negative"]:
                all_csv_rows.append({"label": cls, "cleaned_text": text})
        
        torch.cuda.empty_cache() if DEVICE == "cuda" else None
        gc.collect()
    
    # ä¿å­˜ç»“æœä¸º JSON æ–‡ä»¶
    json_path = os.path.join(OUTPUT_DIR, f'cleaned_data_{filename}.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(all_sentiment_results, f, ensure_ascii=False, indent=2)
    logging.info(f"JSON æ–‡ä»¶ä¿å­˜è‡³: {json_path}")
    
    # ä¿å­˜ç»“æœä¸º CSV æ–‡ä»¶
    if all_csv_rows:
        csv_path = os.path.join(OUTPUT_DIR, f'cleaned_data_{filename}.csv')
        with open(csv_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=['label', "cleaned_text"])
            writer.writeheader()
            writer.writerows(all_csv_rows)
        logging.info(f"CSV æ–‡ä»¶ä¿å­˜è‡³: {csv_path}")

# ä¸»ç¨‹åº
def main():
    json_dir = "/gemini/code/json_files"
    json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]
    if not json_files:
        logging.error("æœªæ‰¾åˆ° JSON æ–‡ä»¶")
        return
    
    for json_file in tqdm(json_files, desc="å¤„ç†æ–‡ä»¶"):
        file_path = os.path.join(json_dir, json_file)
        process_file(file_path)
        gc.collect()

if __name__ == "__main__":
    main()