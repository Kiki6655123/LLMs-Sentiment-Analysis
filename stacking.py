import torch
import numpy as np
import pandas as pd
from transformers import AutoModelForSequenceClassification, AutoTokenizer, MBartForConditionalGeneration, MBart50TokenizerFast
from peft import PeftModel  # å¼•å…¥ peft åº“ä»¥åŠ è½½ LoRA æ¨¡å‹
from sentence_transformers import SentenceTransformer
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import xgboost as xgb
import time
import logging
import os

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/gemini/code/stacking_log.txt', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

# 1. åŠ è½½å¾®è°ƒæ¨¡å‹å’Œåˆ†è¯å™¨
logger.info("å¼€å§‹åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
# åŠ è½½ XLM-RoBERTa
xlm_model = AutoModelForSequenceClassification.from_pretrained("/gemini/code/finetuned_xlm_roberta")
xlm_tokenizer = AutoTokenizer.from_pretrained("/gemini/code/finetuned_xlm_roberta")

# åŠ è½½ DeepSeekï¼ˆåŸºç¡€æ¨¡å‹ + LoRA é€‚é…å™¨ï¼‰
base_model = AutoModelForSequenceClassification.from_pretrained(
    "/gemini/code/DeepSeek-R1-Distill-Qwen-7B",
    trust_remote_code=True,
    num_labels=2  # ç¡®ä¿äºŒåˆ†ç±»
)
deepseek_model = PeftModel.from_pretrained(
    base_model,
    "/gemini/code/instruction_finetuned_deepseek",
    trust_remote_code=True
)
deepseek_tokenizer = AutoTokenizer.from_pretrained(
    "/gemini/code/DeepSeek-R1-Distill-Qwen-7B",
    trust_remote_code=True
)

# å¼ºåˆ¶è®¾ç½® DeepSeek åˆ†è¯å™¨çš„ pad_token å’Œ pad_token_id
if deepseek_tokenizer.pad_token is None:
    deepseek_tokenizer.pad_token = deepseek_tokenizer.eos_token  # ä½¿ç”¨ eos_token ä½œä¸º pad_token
    deepseek_tokenizer.pad_token_id = deepseek_tokenizer.eos_token_id
    logger.info("ä¸º DeepSeek åˆ†è¯å™¨è®¾ç½® pad_token: %s", deepseek_tokenizer.pad_token)

# å¦‚æœä»æœªç”Ÿæ•ˆï¼Œæ‰‹åŠ¨æ·»åŠ  pad_token
if deepseek_tokenizer.pad_token is None:
    deepseek_tokenizer.add_special_tokens({'pad_token': '<pad>'})
    deepseek_tokenizer.pad_token_id = deepseek_tokenizer.convert_tokens_to_ids('<pad>')
    deepseek_model.resize_token_embeddings(len(deepseek_tokenizer))
    logger.info("æ‰‹åŠ¨ä¸º DeepSeek åˆ†è¯å™¨æ·»åŠ  pad_token: <pad>")

# ç¡®ä¿æ¨¡å‹é…ç½®ä¸­åŒ…å« pad_token_id
if not hasattr(deepseek_model.config, 'pad_token_id') or deepseek_model.config.pad_token_id is None:
    deepseek_model.config.pad_token_id = deepseek_tokenizer.pad_token_id
    logger.info("ä¸º DeepSeek æ¨¡å‹è®¾ç½® pad_token_id: %d", deepseek_model.config.pad_token_id)

models = {
    "xlm": xlm_model,
    "deepseek": deepseek_model
}
tokenizers = {
    "xlm": xlm_tokenizer,
    "deepseek": deepseek_tokenizer
}

# åŠ è½½ mBART_finetuned_v5
logger.info("åŠ è½½ mBART æ¨¡å‹...")
mbart_model_path = "/gemini/code/models/mbart_finetuned_v5"
if not os.path.exists(mbart_model_path):
    logger.error(f"mBART æ¨¡å‹è·¯å¾„ {mbart_model_path} ä¸å­˜åœ¨ï¼")
    raise FileNotFoundError(f"mBART æ¨¡å‹è·¯å¾„ {mbart_model_path} ä¸å­˜åœ¨ï¼")
mbart_model = MBartForConditionalGeneration.from_pretrained(mbart_model_path)
mbart_tokenizer = MBart50TokenizerFast.from_pretrained(mbart_model_path)
logger.info("mBART æ¨¡å‹åŠ è½½å®Œæˆã€‚")

# åŠ è½½ SentenceTransformer
sentence_transformer = SentenceTransformer("/gemini/code/sentence_transformer_finetuned")

# å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
for name, model in models.items():
    model.to(device)
    model.eval()
mbart_model.to(device)
mbart_model.eval()
sentence_transformer.to(device)
logger.info("æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½å®Œæˆã€‚")

# 2. åˆå§‹æƒé‡ï¼ˆè°ƒæ•´æƒé‡ä»¥æå‡æ€§èƒ½ï¼‰
weights = {
    "xlm": 0.8,      # XLM-RoBERTa æƒé‡æé«˜
    "deepseek": 0.2   # DeepSeek æƒé‡é™ä½
}
logger.info(f"åˆå§‹æƒé‡: {weights}")

# 3. æ–‡åŒ–è¯å…¸ï¼ˆç®€åŒ–ç‰ˆï¼‰
cultural_dict = {
    "slang": {
        "cant": {"freq": 383, "sentiment": "negative"},
        "bro": {"freq": 111, "sentiment": "neutral"},
        "wont": {"freq": 146, "sentiment": "negative"},
        "idk": {"freq": 42, "sentiment": "neutral"},
        "bruh": {"freq": 17, "sentiment": "negative"},
        "nah": {"freq": 43, "sentiment": "negative"},
        "lol": {"freq": 156, "sentiment": "positive"},
        "lit": {"freq": 4, "sentiment": "positive"},
        "nope": {"freq": 15, "sentiment": "negative"},
        "dude": {"freq": 53, "sentiment": "neutral"},
        "cool": {"freq": 47, "sentiment": "positive"},
        "wtf": {"freq": 35, "sentiment": "negative"}
    },
    "emoji": {
        "ğŸ˜ƒ": {"freq": 8, "sentiment": "positive"},
        "ğŸ˜": {"freq": 46, "sentiment": "positive"},
        "ğŸ®": {"freq": 9, "sentiment": "neutral"},
        "ğŸ”¥": {"freq": 575, "sentiment": "positive"},
        "ğŸ‘": {"freq": 53, "sentiment": "positive"},
        "ğŸ”½": {"freq": 4, "sentiment": "negative"},
        "ğŸŒ±": {"freq": 27, "sentiment": "positive"},
        "ğŸ’¼": {"freq": 104, "sentiment": "neutral"},
        "ğŸ™Œ": {"freq": 20, "sentiment": "positive"},
        "ğŸ’¡": {"freq": 36, "sentiment": "positive"},
        "ğŸ˜‚": {"freq": 293, "sentiment": "positive"},
        "ğŸ˜­": {"freq": 317, "sentiment": "negative"},
        "ğŸ’”": {"freq": 191, "sentiment": "negative"},
        "ğŸ™": {"freq": 128, "sentiment": "neutral"},
        "ğŸ˜¡": {"freq": 66, "sentiment": "negative"},
        "ğŸ’ª": {"freq": 187, "sentiment": "positive"},
        "ğŸ˜Š": {"freq": 54, "sentiment": "positive"}
    }
}
logger.info("æ–‡åŒ–è¯å…¸åŠ è½½å®Œæˆã€‚")

# 4. å•æ¨¡å‹é¢„æµ‹å‡½æ•°
def predict_single_model(model, tokenizer, text, device):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(device)
    with torch.no_grad():
        outputs = model(**inputs).logits.softmax(dim=-1)  # [neg, pos]
    return outputs.cpu().numpy()[0]

# 5. æ‰¹é‡ç¿»è¯‘å‡½æ•°ï¼ˆç§»é™¤ Promptï¼‰
def batch_translate(texts, mbart_model, mbart_tokenizer, device, src_lang="en_XX", tgt_lang="zh_CN"):
    mbart_tokenizer.src_lang = src_lang
    inputs = mbart_tokenizer(texts, return_tensors="pt", truncation=True, padding=True).to(device)
    translated = mbart_model.generate(**inputs, forced_bos_token_id=mbart_tokenizer.lang_code_to_id[tgt_lang], max_length=128)
    result = [mbart_tokenizer.decode(t, skip_special_tokens=True) for t in translated]
    return result

# 6. æ··åˆå¼å›è¯‘å‡½æ•°ï¼ˆç§»é™¤ä¸­é€”å›è¯‘ç»“æœï¼‰
translation_cache = {}
def enhanced_back_translate(texts, mbart_model, mbart_tokenizer, device, cultural_dict, batch_size=16):
    # ä¿æŠ¤æ–‡åŒ–è´Ÿè½½è¯
    processed_texts = []
    for text in texts:
        if text in translation_cache:
            processed_texts.append(text)
            continue
        for slang, info in cultural_dict["slang"].items():
            if slang in text:
                text = text.replace(slang, f"[{slang}]")
        for emoji, info in cultural_dict["emoji"].items():
            if emoji in text:
                text = text.replace(emoji, f"[{emoji}]")
        processed_texts.append(text)
    
    # ç¿»è¯‘åˆ°ä¸­æ–‡
    translated_texts = []
    for i in range(0, len(processed_texts), batch_size):
        batch_texts = processed_texts[i:i + batch_size]
        translated_batch = batch_translate(batch_texts, mbart_model, mbart_tokenizer, device, src_lang="en_XX", tgt_lang="zh_CN")
        translated_texts.extend(translated_batch)
        torch.cuda.empty_cache()  # é‡Šæ”¾ GPU å†…å­˜
    
    # å›è¯‘åˆ°è‹±æ–‡
    result_texts = []
    for i in range(0, len(translated_texts), batch_size):
        batch_texts = translated_texts[i:i + batch_size]
        back_translated_batch = batch_translate(batch_texts, mbart_model, mbart_tokenizer, device, src_lang="zh_CN", tgt_lang="en_XX")
        result_texts.extend(back_translated_batch)
        torch.cuda.empty_cache()  # é‡Šæ”¾ GPU å†…å­˜
    
    # æ¢å¤æ–‡åŒ–è´Ÿè½½è¯å¹¶å¢å¼ºæƒ…æ„Ÿ
    final_texts = []
    for result in result_texts:
        for slang, info in cultural_dict["slang"].items():
            if f"[{slang}]" in result:
                sentiment = info["sentiment"]
                result = result.replace(f"[{slang}]", f"{slang} ({'ç§¯æ' if sentiment == 'positive' else 'æ¶ˆæ' if sentiment == 'negative' else 'ä¸­æ€§'})")
        for emoji, info in cultural_dict["emoji"].items():
            if f"[{emoji}]" in result:
                sentiment = info["sentiment"]
                result = result.replace(f"[{emoji}]", f"{emoji} ({'ç§¯æ' if sentiment == 'positive' else 'æ¶ˆæ' if sentiment == 'negative' else 'ä¸­æ€§'})")
        final_texts.append(result)
    
    return final_texts

# 7. è·å– SentenceTransformer åµŒå…¥
def get_sentence_embeddings(texts, sentence_transformer, batch_size=32):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        batch_embeddings = sentence_transformer.encode(batch_texts, convert_to_numpy=True)
        embeddings.extend(batch_embeddings)
    return np.array(embeddings)

# 8. åŠ æƒæŠ•ç¥¨é¢„æµ‹å‡½æ•°
def predict_ensemble(text, models, tokenizers, weights, device, use_enhanced_translation=False):
    if use_enhanced_translation:
        text = enhanced_back_translate([text], mbart_model, mbart_tokenizer, device, cultural_dict)[0]
    
    preds = {}
    for name in models:
        preds[name] = predict_single_model(models[name], tokenizers[name], text, device)
    
    final_pred = np.sum([weights[name] * preds[name] for name in models], axis=0)
    label = "ç§¯æ" if final_pred[1] > final_pred[0] else "æ¶ˆæ"
    return label, final_pred, text

# 9. Stacking é›†æˆé¢„æµ‹å‡½æ•°ï¼ˆç»“åˆ SentenceTransformer åµŒå…¥ï¼‰
def predict_stacking(texts, models, tokenizers, sentence_transformer, device, batch_size=256, use_enhanced_translation=False):
    all_probs = {"xlm": [], "deepseek": []}
    
    if use_enhanced_translation:
        texts = enhanced_back_translate(texts, mbart_model, mbart_tokenizer, device, cultural_dict, batch_size=16)
    
    # è·å–æ¨¡å‹é¢„æµ‹æ¦‚ç‡
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        for name in models:
            inputs = tokenizers[name](
                batch_texts,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=512
            ).to(device)
            with torch.no_grad():
                outputs = models[name](**inputs).logits.softmax(dim=-1).cpu().numpy()
            all_probs[name].extend(outputs)
        torch.cuda.empty_cache()  # é‡Šæ”¾ GPU å†…å­˜
    
    # è·å– SentenceTransformer åµŒå…¥
    embeddings = get_sentence_embeddings(texts, sentence_transformer, batch_size=32)
    
    # åˆå¹¶ç‰¹å¾ï¼ˆæ¨¡å‹æ¦‚ç‡ + åµŒå…¥ï¼‰
    X = np.hstack([np.array(all_probs["xlm"]), np.array(all_probs["deepseek"]), embeddings])
    meta_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', max_depth=12, learning_rate=0.02, n_estimators=500)
    meta_model.fit(X[:len(test_labels)], test_labels)  # ä½¿ç”¨æµ‹è¯•é›†æ ‡ç­¾è®­ç»ƒå…ƒæ¨¡å‹
    preds = meta_model.predict(X)
    return ["ç§¯æ" if p == 1 else "æ¶ˆæ" for p in preds], X

# 10. æ‰¹é‡é¢„æµ‹å‡½æ•°ï¼ˆåŠ æƒæŠ•ç¥¨ï¼‰
def predict_batch(texts, models, tokenizers, weights, device, batch_size=256, use_enhanced_translation=False):
    all_labels = {"xlm": [], "deepseek": [], "ensemble": []}
    all_probs = {"xlm": [], "deepseek": [], "ensemble": []}
    enhanced_texts = []
    
    start_time = time.time()
    if use_enhanced_translation:
        texts = enhanced_back_translate(texts, mbart_model, mbart_tokenizer, device, cultural_dict, batch_size=16)
        enhanced_texts.extend(texts)
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        
        batch_preds = {name: [] for name in models}
        for name in models:
            inputs = tokenizers[name](
                batch_texts,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=512
            ).to(device)
            with torch.no_grad():
                outputs = models[name](**inputs).logits.softmax(dim=-1).cpu().numpy()
            batch_preds[name] = outputs
        torch.cuda.empty_cache()  # é‡Šæ”¾ GPU å†…å­˜
        
        for j in range(len(batch_texts)):
            for name in models:
                pred = batch_preds[name][j]
                label = "ç§¯æ" if pred[1] > pred[0] else "æ¶ˆæ"
                all_labels[name].append(label)
                all_probs[name].append(pred)
            final_pred = np.sum([weights[name] * batch_preds[name][j] for name in models], axis=0)
            label = "ç§¯æ" if final_pred[1] > final_pred[0] else "æ¶ˆæ"
            all_labels["ensemble"].append(label)
            all_probs["ensemble"].append(final_pred)
    
    end_time = time.time()
    logger.info(f"æ¨ç† {len(texts)} ä¸ªæ ·æœ¬è€—æ—¶: {end_time - start_time:.2f} ç§’")
    return all_labels, all_probs, enhanced_texts if use_enhanced_translation else None

# 11. ä¸»å‡½æ•°ï¼šåŠ è½½æ•°æ®ã€é¢„æµ‹ä¸å¯¹æ¯”
def main():
    global test_labels  # å…¨å±€å˜é‡ï¼Œç”¨äº Stacking
    # åŠ è½½æµ‹è¯•æ•°æ®é›†å¹¶ç¡®ä¿äºŒåˆ†ç±»
    logger.info("å¼€å§‹åŠ è½½æµ‹è¯•æ•°æ®é›†...")
    data = pd.read_csv("/gemini/code/new_tweets_v2.csv")
    data = data[data["label"] != 2].reset_index(drop=True)
    data = data.dropna(subset=['cleaned_text'])  # æ¸…æ´— NaN å€¼
    logger.info("æµ‹è¯•æ•°æ®é›†åŠ è½½å®Œæˆï¼ˆäºŒåˆ†ç±»ï¼‰ã€‚æ ·æœ¬é¢„è§ˆ:")
    logger.info(f"\n{data.head().to_string()}")
    logger.info("æ ‡ç­¾åˆ†å¸ƒ:")
    logger.info(f"\n{data['label'].value_counts().to_string()}")

    # æµ‹è¯•é›†è¯„ä¼°
    test_texts = data["cleaned_text"].tolist()
    test_labels = data["label"].values

    # å•æ¡é¢„æµ‹ç¤ºä¾‹
    sample_text = "blockchain is lit ğŸ”¥ and bro this is cool ğŸ‘"
    label, probs, enhanced = predict_ensemble(sample_text, models, tokenizers, weights, device, use_enhanced_translation=True)
    logger.info("\nå•æ¡é¢„æµ‹ï¼ˆä½¿ç”¨å¢å¼ºç¿»è¯‘ï¼‰:")
    logger.info(f"åŸå§‹æ–‡æœ¬: {sample_text}")
    logger.info(f"å¢å¼ºæ–‡æœ¬: {enhanced}")
    logger.info(f"é¢„æµ‹ç»“æœ: {label}, æ¦‚ç‡: {probs}")

    # ä¸ä½¿ç”¨å¢å¼ºç¿»è¯‘ï¼ˆåŠ æƒæŠ•ç¥¨ï¼‰
    logger.info("\næµ‹è¯•é›†æ€§èƒ½ï¼ˆä¸ä½¿ç”¨å¢å¼ºç¿»è¯‘ï¼ŒåŠ æƒæŠ•ç¥¨ï¼‰:")
    labels, probs, _ = predict_batch(test_texts, models, tokenizers, weights, device, use_enhanced_translation=False)
    for model in ["xlm", "deepseek", "ensemble"]:
        pred_labels = [1 if p == "ç§¯æ" else 0 for p in labels[model]]
        logger.info(f"\n{model.upper()} ç»“æœ:")
        logger.info(f"å‡†ç¡®ç‡: {accuracy_score(test_labels, pred_labels):.4f}")
        logger.info(f"F1 åˆ†æ•°: {f1_score(test_labels, pred_labels):.4f}")
        logger.info(f"ç²¾ç¡®ç‡: {precision_score(test_labels, pred_labels):.4f}")
        logger.info(f"å¬å›ç‡: {recall_score(test_labels, pred_labels):.4f}")

    # ä½¿ç”¨å¢å¼ºç¿»è¯‘ï¼ˆåŠ æƒæŠ•ç¥¨ï¼‰
    logger.info("\næµ‹è¯•é›†æ€§èƒ½ï¼ˆä½¿ç”¨å¢å¼ºç¿»è¯‘ï¼ŒåŠ æƒæŠ•ç¥¨ï¼‰:")
    labels_enhanced, probs_enhanced, enhanced_texts = predict_batch(test_texts, models, tokenizers, weights, device, use_enhanced_translation=True)
    for model in ["xlm", "deepseek", "ensemble"]:
        pred_labels = [1 if p == "ç§¯æ" else 0 for p in labels_enhanced[model]]
        logger.info(f"\n{model.upper()} ç»“æœ:")
        logger.info(f"å‡†ç¡®ç‡: {accuracy_score(test_labels, pred_labels):.4f}")
        logger.info(f"F1 åˆ†æ•°: {f1_score(test_labels, pred_labels):.4f}")
        logger.info(f"ç²¾ç¡®ç‡: {precision_score(test_labels, pred_labels):.4f}")
        logger.info(f"å¬å›ç‡: {recall_score(test_labels, pred_labels):.4f}")

    # ä½¿ç”¨å¢å¼ºç¿»è¯‘ï¼ˆStacking é›†æˆï¼‰
    logger.info("\næµ‹è¯•é›†æ€§èƒ½ï¼ˆä½¿ç”¨å¢å¼ºç¿»è¯‘ï¼ŒStacking é›†æˆï¼‰:")
    labels_stacking, probs_stacking = predict_stacking(test_texts, models, tokenizers, sentence_transformer, device, use_enhanced_translation=True)
    pred_labels = [1 if p == "ç§¯æ" else 0 for p in labels_stacking]
    logger.info("\nSTACKING ç»“æœ:")
    logger.info(f"å‡†ç¡®ç‡: {accuracy_score(test_labels, pred_labels):.4f}")
    logger.info(f"F1 åˆ†æ•°: {f1_score(test_labels, pred_labels):.4f}")
    logger.info(f"ç²¾ç¡®ç‡: {precision_score(test_labels, pred_labels):.4f}")
    logger.info(f"å¬å›ç‡: {recall_score(test_labels, pred_labels):.4f}")

if __name__ == "__main__":
    main()