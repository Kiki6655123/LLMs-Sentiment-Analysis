import pandas as pd
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, Seq2SeqTrainer, Seq2SeqTrainingArguments
import torch
import nltk
import os
import emoji
import logging
from datasets import Dataset
from torch.optim import AdamW
import re

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# è®¾ç½® NLTK æ•°æ®è·¯å¾„
nltk.data.path.append('/root/nltk_data')
for resource in ['punkt', 'punkt_tab']:
    if not os.path.exists(f'/root/nltk_data/tokenizers/{resource}'):
        raise FileNotFoundError(f"NLTK èµ„æº {resource} æœªæ‰¾åˆ°")

# é¢„å¤„ç†å‡½æ•°
def preprocess_emoji(text):
    emoji_dict = {'ğŸ‘': ' positive ', 'ğŸ‘': ' negative ', 'ğŸ˜Š': ' happy ', 'â¤ï¸': ' love ', 'ğŸŒŸ': ' star ', 'ğŸ˜ƒ': ''}
    for emoji_char, replacement in emoji_dict.items():
        text = text.replace(emoji_char, replacement)
    text = emoji.replace_emoji(text, replace='')
    return text

def preprocess_text(text):
    if not isinstance(text, str):
        text = str(text)
    text = preprocess_emoji(text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†
def prepare_dataset(corpus_path, document_path, sample_size=20000):
    logging.info("åŠ è½½ cleaned_news_commentary æ•°æ®é›†...")
    news_df = pd.read_csv(corpus_path, sep='\t', names=['source', 'target'], on_bad_lines='skip')
    news_df = news_df.sample(n=sample_size, random_state=42)  # éšæœºæŠ½æ · 20000 æ¡
    news_df['source'] = news_df['source'].apply(preprocess_text)
    news_df['target'] = news_df['target'].apply(preprocess_text)

    logging.info("ä»æ–‡æ¡£ä¸­æå–å¹³è¡Œè¯­æ–™...")
    with open(document_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    doc_parallel_data = []
    for line in lines:
        line = line.strip()
        if line.startswith('<DOCUMENT>') or line.startswith('</DOCUMENT>'):
            continue
        if '\t' in line:
            en_text, zh_text = line.split('\t', 1)
            if en_text and zh_text:
                doc_parallel_data.append({"source": preprocess_text(en_text), "target": preprocess_text(zh_text)})

    doc_df = pd.DataFrame(doc_parallel_data)

    combined_df = pd.concat([news_df, doc_df], ignore_index=True)
    logging.info(f"åˆå¹¶åçš„æ•°æ®é›†å¤§å°: {len(combined_df)}")

    total_size = len(combined_df)
    train_size = int(0.8 * total_size)
    val_size = int(0.1 * total_size)
    test_size = total_size - train_size - val_size

    train_data = {"source": combined_df['source'][:train_size].tolist(), "target": combined_df['target'][:train_size].tolist()}
    val_data = {"source": combined_df['source'][train_size:train_size + val_size].tolist(), "target": combined_df['target'][train_size:train_size + val_size].tolist()}
    test_data = {"source": combined_df['source'][train_size + val_size:].tolist(), "target": combined_df['target'][train_size + val_size:].tolist()}

    train_dataset = Dataset.from_dict(train_data)
    val_dataset = Dataset.from_dict(val_data)
    test_dataset = Dataset.from_dict(test_data)

    combined_df.to_csv('/gemini/code/combined_parallel_corpus_20000.csv', index=False)
    test_dataset.save_to_disk('/gemini/code/test_dataset_20000')
    logging.info("åˆå¹¶åçš„å¹³è¡Œè¯­æ–™å·²ä¿å­˜è‡³ /gemini/code/combined_parallel_corpus_20000.csv")
    logging.info("æµ‹è¯•æ•°æ®é›†å·²ä¿å­˜è‡³ /gemini/code/test_dataset_20000")

    tokenizer = MBart50TokenizerFast.from_pretrained("/gemini/code/models/mbart-large-50-many-to-many-mmt")
    return train_dataset, val_dataset, test_dataset, tokenizer

# æ•°æ®é›†é¢„å¤„ç†
def preprocess_function(examples, tokenizer, max_length=512):
    inputs = examples["source"]
    targets = examples["target"]
    tokenizer.src_lang = "en_XX"
    tokenizer.tgt_lang = "zh_CN"
    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding="max_length", return_tensors="pt")
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_length, truncation=True, padding="max_length", return_tensors="pt")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# å¾®è°ƒ mBART
def finetune_mbart(train_dataset, val_dataset, model_path="/gemini/code/models/mbart_finetuned_v3", output_dir="/gemini/code/models/mbart_finetuned_v5"):
    logging.info("åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    model = MBartForConditionalGeneration.from_pretrained(model_path)
    tokenizer = MBart50TokenizerFast.from_pretrained(model_path)

    # å†»ç»“ç¼–ç å™¨å‚æ•°
    logging.info("å†»ç»“ç¼–ç å™¨å‚æ•°...")
    for param in model.model.encoder.parameters():
        param.requires_grad = False

    # å¢åŠ  dropout
    logging.info("å¢åŠ  dropout...")
    model.config.dropout = 0.3  # é»˜è®¤ 0.1ï¼Œå¢åŠ åˆ° 0.3

    logging.info("é¢„å¤„ç†æ•°æ®é›†...")
    tokenized_train = train_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)
    tokenized_val = val_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)

    tokenized_train.save_to_disk('/gemini/code/tokenized_train_dataset_v5_20000')
    tokenized_val.save_to_disk('/gemini/code/tokenized_val_dataset_v5_20000')
    logging.info("é¢„å¤„ç†åçš„æ•°æ®é›†å·²ä¿å­˜è‡³ /gemini/code/tokenized_*_dataset_v5_20000")

    logging.info("è®¾ç½®è®­ç»ƒå‚æ•°...")
    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        eval_strategy="epoch",
        learning_rate=1e-5,  # é™ä½å­¦ä¹ ç‡
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=2,  # å‡å°‘è®­ç»ƒè½®æ•°
        weight_decay=0.2,  # å¢åŠ æ­£åˆ™åŒ–
        save_total_limit=2,
        save_steps=1000,
        logging_steps=500,
        fp16=True,
        use_cpu=False,
    )

    optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.2)
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        optimizers=(optimizer, None),
    )

    logging.info("å¼€å§‹å¾®è°ƒ...")
    trainer.train()

    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    logging.info(f"å¾®è°ƒæ¨¡å‹å·²ä¿å­˜è‡³ {output_dir}")
    return model, tokenizer

if __name__ == "__main__":
    logging.info(f"GPU å¯ç”¨: {torch.cuda.is_available()}")
    document_path = '/gemini/code/document.txt'
    corpus_path = '/gemini/code/cleaned_news_commentary.tsv'

    with open(document_path, 'w', encoding='utf-8') as f:
        f.write("""<DOCUMENT>
Countries, companies, and others worldwide have committed to eliminating their net greenhouse-gas emissions by a particular date â€“ for some, as early as 2030.	ä¸–ç•Œå„åœ°çš„å›½å®¶ã€ä¼ä¸šå’Œå…¶ä»–å›½å®¶éƒ½æ‰¿è¯ºè¦åœ¨æŸä¸ªç‰¹å®šæ—¥æœŸå‰æ¶ˆé™¤æ¸©å®¤æ°”ä½“å‡€æ’æ”¾ â€” â€” æŸäº›å›½å®¶çš„è®¾å®šæ—©åˆ°2030å¹´ã€‚
A 2021 report by the International Energy Agency, for example, charts a detailed path, divided into five-year intervals, toward achieving net-zero emissions by 2050 â€“ and giving the world â€œan even chance of limiting the global temperature rise to 1.5Â°C.â€	æ¯”å¦‚å›½é™…èƒ½æºç½²äº2021å¹´å‘è¡¨çš„ä¸€ä»½æŠ¥å‘Šå°±æç»˜äº†ä¸€æ¡ä»¥äº”å¹´ä¸ºé—´éš”çš„è¯¦ç»†è·¯å¾„ï¼Œè®¡åˆ’åœ¨2050å¹´å®ç°å‡€é›¶æ’æ”¾ï¼Œå¹¶ç»™ä¸–ç•Œâ€œä¸€ä¸ªå‡ç­‰çš„æœºä¼šå»å°†å…¨çƒæ°”æ¸©ä¸Šå‡å¹…åº¦æ§åˆ¶åœ¨1.5Â°Cä»¥å†… â€ ã€‚
The most striking feature of this analysis, at least to me, is the magnitude of the decline that is required by 2030: roughly eight billion tons of fossil-fuel-based emissions, taking us from the 34 gigatons carbon dioxide today to 26 Gt.	è¿™ç¯‡åˆ†ææœ€æ˜¾è‘—çš„è¦ç‚¹ â€” â€” è‡³å°‘åœ¨æˆ‘çœ‹æ¥ â€” â€” æ˜¯åˆ°2030å¹´æ‰€éœ€çš„å‡æ’å¹…åº¦ï¼šå¤§çº¦80äº¿å¨åŒ–çŸ³ç‡ƒæ–™ç›¸å…³æ’æ”¾ï¼Œä½¿æˆ‘ä»¬ä»å½“å‰çš„340äº¿å¨äºŒæ°§åŒ–ç¢³æ’æ”¾å‡å°‘åˆ°260äº¿å¨ã€‚
If the global economy grows at a conservatively estimated annual rate of 2% over that period, the global economyâ€™s carbon intensity (CO2 emissions per $1,000 of GDP) would need to decline by 7.8% per year.	å¦‚æœå…¨çƒç»æµåœ¨æ­¤æœŸé—´ä»¥ä¿å®ˆä¼°è®¡çš„2%å¹´å¢é•¿ç‡å¢é•¿ï¼Œå…¨çƒç»æµçš„ç¢³å¼ºåº¦ï¼ˆæ¯åˆ›é€ 1000ç¾å…ƒGDPçš„äºŒæ°§åŒ–ç¢³æ’æ”¾é‡ï¼‰éœ€è¦æ¯å¹´ä¸‹é™7.8 % ã€‚
While carbon intensity has been declining over the last 40 years, the trend has been nowhere near this rate: from 1980 to 2021, carbon intensity fell by just 1.3% per year, on average.	è™½ç„¶è¯¥å¼ºåº¦åœ¨è¿‡å»40å¹´é—´ä¸€ç›´åœ¨ä¸‹é™ï¼Œä½†è¿™ä¸€è¶‹åŠ¿è¿œæœªè¾¾åˆ°ä¸Šè¿°é€Ÿåº¦ï¼š1980~2021å¹´é—´çš„ç¢³å¼ºåº¦å¹³å‡æ¯å¹´ä»…ä¸‹é™1.3 % ã€‚
The decline that occurred was largely a byproduct of emerging economies becoming wealthier. (More developed economies have lower carbon intensities.)	è¿™ç§ä¸‹é™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯æ–°å…´ç»æµä½“å˜å¾—æ›´åŠ å¯Œè£•çš„å‰¯äº§å“ï¼ˆè¶Šå‘è¾¾çš„ç»æµä½“ç¢³å¼ºåº¦è¶Šä½ ï¼‰ ã€‚
To be sure, as climate change gained more attention from policymakers, the rate of decline did accelerate, averaging 1.9% per year since 2010.	å¯ä»¥è‚¯å®šçš„æ˜¯ï¼Œéšç€æ°”å€™å˜åŒ–å¾—åˆ°æ”¿ç­–åˆ¶å®šè€…çš„æ›´å¤šå…³æ³¨ï¼Œä¸‹é™é€Ÿåº¦ç¡®å®åŠ å¿«äº†ï¼Œè‡ª2010å¹´ä»¥æ¥å¹³å‡æ¯å¹´ä¸‹é™1.9 % ã€‚
And with supply-side constraints now encumbering the global economy â€“ annual growth could well run at just 2% in the next few years â€“ a modest further reduction in carbon intensity could be enough to put the global economy at or near the peak of its total CO2 emissions.	é‰´äºå„ç±»ä¾›ç»™ä¾§é™åˆ¶å¦‚ä»Šå›°æ‰°ç€å…¨çƒç»æµ â€” â€” æœªæ¥å‡ å¹´çš„å¹´å¢é•¿ç‡å¾ˆå¯èƒ½åªæœ‰2 % â€” â€”ç¢³æ’æ”¾å¼ºåº¦çš„è¿›ä¸€æ­¥å°å¹…é™ä½æˆ–è®¸è¶³ä»¥ä½¿å…¨çƒç»æµè¾¾åˆ°æˆ–æ¥è¿‘å…¶äºŒæ°§åŒ–ç¢³æ’æ”¾æ€»é‡å³°å€¼ã€‚
Higher global growth might not even set back efforts to reduce the economyâ€™s carbon intensity, if it is fueled by the proliferation of digital technologies.	å¦‚æœæ˜¯ç”±æ•°å­—æŠ€æœ¯çš„æ‰©æ•£æ¨åŠ¨çš„è¯ï¼Œæ›´é«˜çš„å…¨çƒå¢é•¿ç”šè‡³å¯èƒ½ä¸ä¼šé˜»ç¢é™ä½ç»æµç¢³å¼ºåº¦çš„åŠªåŠ› ï¼Œ ã€‚
Or is it worse to acquiesce to the consequences of abandoning the ambitious path, including the risk of crossing irreversible tipping points?	æˆ–è€…é»˜é»˜æ¥å—æ”¾å¼ƒè¿™æ¡é›„å¿ƒå‹ƒå‹ƒè·¯å¾„çš„æ‰€æœ‰åæœ â€” â€” åŒ…æ‹¬è·¨è¶Šä¸å¯é€†è½¬ä¸´ç•Œç‚¹çš„é£é™© â€” â€” æ˜¯ä¸æ˜¯æ›´ç³Ÿç³•ï¼Ÿ
Half of global greenhouse-gas emissions come from just seven economies: China, the United States, the European Union, Japan, India, Canada, Australia, and Russia. The G20 economies account for 70%.	å…¨çƒæ¸©å®¤æ°”ä½“æ’æ”¾é‡åŠæ•°æ¥è‡ªäº7ä¸ªç»æµä½“ï¼šä¸­å›½ã€ç¾å›½ã€æ¬§ç›Ÿã€æ—¥æœ¬ã€å°åº¦ã€åŠ æ‹¿å¤§ï¼Œæ¾³å¤§åˆ©äºšå’Œä¿„ç½—æ–¯ï¼Œè€ŒG20ç»æµä½“åˆ™å æ®äº†70 % ã€‚
The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).	ç›®å‰çš„è¶‹åŠ¿æ˜¯ï¼Œè¦ä¹ˆæ˜¯è¿‡åº¦çš„å…‹åˆ¶ï¼ˆæ¬§æ´² ï¼‰ ï¼Œ è¦ä¹ˆæ˜¯åŠªåŠ›çš„æ‰©å±•ï¼ˆç¾å›½ ï¼‰ ã€‚
</DOCUMENT>""")
    logging.info("æ–‡æ¡£å·²ä¿å­˜è‡³ /gemini/code/document.txt")

    logging.info("å‡†å¤‡æ•°æ®é›†...")
    train_dataset, val_dataset, test_dataset, tokenizer = prepare_dataset(corpus_path, document_path)

    logging.info("å¼€å§‹å¾®è°ƒ mBART...")
    finetuned_model, finetuned_tokenizer = finetune_mbart(train_dataset, val_dataset)